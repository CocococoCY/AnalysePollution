import org.apache.spark.sql.SparkSession

object Etape2_RDD_Basics {

  def main(args: Array[String]): Unit = {

    // CrÃ©er la SparkSession
    val spark = SparkSession.builder()
      .appName("Ã‰tape 2 - RDD Basics")
      .master("local[*]")
      .config("spark.hadoop.home.dir", "/")
      .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    println("\n" + "=" * 70)
    println("ğŸ“ Ã‰TAPE 2 : Apprendre les RDD et la programmation fonctionnelle")
    println("=" * 70)

    // ========== PARTIE 1 : QU'EST-CE QU'UN RDD ? ==========
    println("\nğŸ“š PARTIE 1 : CrÃ©er ton premier RDD")
    println("-" * 70)

    // Un RDD = Resilient Distributed Dataset
    // C'est une collection distribuÃ©e et immuable

    // CrÃ©er un RDD Ã  partir d'une liste (donnÃ©es de pollution simulÃ©es)
    val mesuresCO2 = Seq(450.5, 520.3, 380.2, 610.8, 490.1, 720.5, 410.0, 580.3)
    val rdd = spark.sparkContext.parallelize(mesuresCO2)

    println("âœ… RDD crÃ©Ã© avec des mesures de CO2 (en ppm)")
    println(s"   Nombre de mesures : ${rdd.count()}")
    println(s"   PremiÃ¨res valeurs : ${rdd.take(3).mkString(", ")}")

    // ========== PARTIE 2 : MAP (Transformer chaque Ã©lÃ©ment) ==========
    println("\nğŸ“š PARTIE 2 : MAP - Transformer chaque Ã©lÃ©ment")
    println("-" * 70)

    // MAP : Applique une fonction Ã  CHAQUE Ã©lÃ©ment
    // Exemple : Convertir ppm en mg/mÂ³ (CO2)
    val rddEnMg = rdd.map(ppm => ppm * 1.8)  // Formule simplifiÃ©e

    println("ğŸ”„ MAP : Conversion ppm â†’ mg/mÂ³")
    println("   AVANT map :")
    rdd.take(3).foreach(v => println(f"   - $v%.1f ppm"))
    println("   APRÃˆS map :")
    rddEnMg.take(3).foreach(v => println(f"   - $v%.1f mg/mÂ³"))

    // Autre exemple MAP : Classifier les niveaux
    val rddClassification = rdd.map { co2 =>
      if (co2 < 450) "ğŸŸ¢ Bon"
      else if (co2 < 600) "ğŸŸ¡ Moyen"
      else "ğŸ”´ Mauvais"
    }

    println("\nğŸ”„ MAP : Classification des niveaux")
    rdd.zip(rddClassification).take(5).foreach { case (co2, classe) =>
      println(f"   CO2: $co2%.1f ppm â†’ $classe")
    }

    // ========== PARTIE 3 : FILTER (SÃ©lectionner des Ã©lÃ©ments) ==========
    println("\nğŸ“š PARTIE 3 : FILTER - SÃ©lectionner des Ã©lÃ©ments")
    println("-" * 70)

    // FILTER : Garde seulement les Ã©lÃ©ments qui satisfont une condition
    val pollutionElevee = rdd.filter(co2 => co2 > 500)

    println(s"ğŸ” FILTER : Garder seulement CO2 > 500 ppm")
    println(s"   Total de mesures : ${rdd.count()}")
    println(s"   Mesures > 500 ppm : ${pollutionElevee.count()}")
    println("   Valeurs filtrÃ©es :")
    pollutionElevee.collect().foreach(v => println(f"   - $v%.1f ppm"))

    // ========== PARTIE 4 : REDUCE (AgrÃ©ger en une valeur) ==========
    println("\nğŸ“š PARTIE 4 : REDUCE - AgrÃ©ger toutes les valeurs")
    println("-" * 70)

    // REDUCE : Combine tous les Ã©lÃ©ments pour obtenir UNE seule valeur

    // Exemple 1 : Trouver le maximum
    val maxCO2 = rdd.reduce((a, b) => if (a > b) a else b)
    println(s"ğŸ“Š REDUCE : Maximum CO2 = $maxCO2 ppm")

    // Exemple 2 : Calculer la somme
    val sommeCO2 = rdd.reduce((a, b) => a + b)
    println(s"ğŸ“Š REDUCE : Somme totale = ${sommeCO2.toInt} ppm")

    // Exemple 3 : Calculer la moyenne (somme / count)
    val moyenneCO2 = sommeCO2 / rdd.count()
    println(f"ğŸ“Š REDUCE : Moyenne = $moyenneCO2%.1f ppm")

    // ========== PARTIE 5 : CHAÃNER LES OPÃ‰RATIONS ==========
    println("\nğŸ“š PARTIE 5 : CHAÃNER les opÃ©rations (le vrai pouvoir !)")
    println("-" * 70)

    // On peut enchaÃ®ner map, filter, reduce !
    val pollutionCritique = rdd
      .filter(co2 => co2 > 600)        // Garder seulement > 600
      .map(co2 => co2 * 1.8)           // Convertir en mg/mÂ³
      .reduce((a, b) => a + b)         // Sommer

    println("ğŸ”— ChaÃ®nage : filter â†’ map â†’ reduce")
    println(s"   RÃ©sultat : Somme des valeurs > 600 ppm (en mg/mÂ³) = ${pollutionCritique.toInt}")

    // ========== PARTIE 6 : EXEMPLE RÃ‰ALISTE ==========
    println("\nğŸ“š PARTIE 6 : Exemple avec des donnÃ©es de stations")
    println("-" * 70)

    // CrÃ©er des donnÃ©es plus rÃ©alistes : (Station, CO2)
    val donnees = Seq(
      ("Station_A", 450.5),
      ("Station_B", 620.3),
      ("Station_A", 480.2),
      ("Station_C", 710.8),
      ("Station_B", 590.1),
      ("Station_A", 420.5),
      ("Station_C", 680.0)
    )

    val rddStations = spark.sparkContext.parallelize(donnees)

    // Question : Quelle est la pollution moyenne de Station_A ?
    val stationA_moyenne = rddStations
      .filter { case (station, co2) => station == "Station_A" }  // Garder Station_A
      .map { case (station, co2) => co2 }                        // Extraire juste CO2
      .reduce((a, b) => a + b) / 3                               // Moyenne (3 mesures)

    println(f"ğŸ­ Pollution moyenne Station_A : $stationA_moyenne%.1f ppm")

    // Question : Combien de stations ont dÃ©passÃ© 600 ppm ?
    val stationsProbleme = rddStations
      .filter { case (station, co2) => co2 > 600 }
      .map { case (station, co2) => station }
      .distinct()  // Enlever les doublons
      .count()

    println(s"âš ï¸  Nombre de stations ayant dÃ©passÃ© 600 ppm : $stationsProbleme")

    // ========== RÃ‰SUMÃ‰ ==========
    println("\n" + "=" * 70)
    println("ğŸ“ RÃ‰SUMÃ‰ DES CONCEPTS")
    println("=" * 70)
    println("""
    âœ… RDD = Collection distribuÃ©e et immuable
    
    âœ… MAP    : Transforme CHAQUE Ã©lÃ©ment
                Exemple : rdd.map(x => x * 2)
    
    âœ… FILTER : Garde seulement certains Ã©lÃ©ments
                Exemple : rdd.filter(x => x > 100)
    
    âœ… REDUCE : Combine tous les Ã©lÃ©ments en UN seul
                Exemple : rdd.reduce((a, b) => a + b)
    
    âœ… On peut CHAÃNER : rdd.filter(...).map(...).reduce(...)
    
    ğŸ’¡ Tout est LAZY : Spark ne calcule que quand on appelle
       une ACTION (count, collect, reduce, take...)
    """)

    println("\nğŸ‰ Bravo ! Tu maÃ®trises maintenant les bases des RDD !")
    println("   Prochaine Ã©tape : GroupBy et les opÃ©rations avancÃ©es !")

    spark.stop()
  }
}